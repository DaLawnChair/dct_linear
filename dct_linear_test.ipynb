{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.fft \n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch_dct import dct, idct  # pip install torch-dct\n",
    "\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the layers\n",
    "\n",
    "class LinearDCT(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear layer with weights trained in DCT domain.\n",
    "    During forward pass, IDCT is applied to recover real-space weights.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        # DCT-domain weights (learned)\n",
    "        self.weight_dct = nn.Parameter(torch.randn(out_features, in_features))\n",
    "\n",
    "        # Optional bias\n",
    "        self.bias = nn.Parameter(torch.randn(out_features)) if bias else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convert DCT weights back to real-space via IDCT\n",
    "        weight_real = idct(self.weight_dct, norm='ortho')  # Shape: [out_features, in_features]\n",
    "\n",
    "        # Apply linear transformation\n",
    "        output = x @ weight_real.T\n",
    "\n",
    "        if self.bias is not None:\n",
    "            output += self.bias\n",
    "\n",
    "        return output\n",
    "    \n",
    "\n",
    "\n",
    "class LinearDCTModel(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(LinearDCT(in_features, in_features//2, bias), LinearDCT(in_features//2, out_features, bias))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, in_features)\n",
    "        return self.model(x)\n",
    "    \n",
    "class LinearStandardModel(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(nn.Linear(in_features, in_features//2, bias), nn.Linear(in_features//2, out_features, bias))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, in_features)\n",
    "        return self.model(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_true_samples(input_size, output_size, instances, seed=0):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    # Random inputs\n",
    "    x = torch.randn(instances, input_size)\n",
    "\n",
    "    # True weight and bias\n",
    "    W = torch.randn(output_size, input_size)\n",
    "    b = torch.randn(output_size)\n",
    "\n",
    "    # True function: y = x @ W.T + b\n",
    "    y = x @ W.T + b  # shape: (instances, output_size)\n",
    "\n",
    "    return x, y, (W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([4, 500])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "in_features = 1000\n",
    "out_features = 500\n",
    "\n",
    "x = torch.randn(batch_size, in_features)\n",
    "\n",
    "layer = LinearDCT(in_features, out_features)\n",
    "output = layer(x)\n",
    "\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE  = 1024\n",
    "OUTPUT_SIZE = 512\n",
    "TRAIN_SAMPLES = 16_000\n",
    "VAL_SAMPLES   = 2_000\n",
    "BATCH_SIZE    = 128\n",
    "EPOCHS        = 100\n",
    "LR            = 1e-3\n",
    "DEVICE        = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "x_train, y_train, _ = generate_true_samples(INPUT_SIZE, OUTPUT_SIZE, TRAIN_SAMPLES, seed=0)\n",
    "x_val,   y_val, _   = generate_true_samples(INPUT_SIZE, OUTPUT_SIZE, VAL_SAMPLES,   seed=1)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(x_train, y_train), batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(TensorDataset(x_val,   y_val),   batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_epoch(model, loader, training=False):\n",
    "    if training:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.set_grad_enabled(training):\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            preds  = model(xb)\n",
    "            loss   = criterion(preds, yb)\n",
    "\n",
    "            if training:\n",
    "                optim.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "\n",
    "            running_loss += loss.item() * xb.size(0)\n",
    "    return running_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "def train(model, train_loader, val_loader, epochs, optim, criterion):\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = run_epoch(model, train_loader, training=True)\n",
    "        val_loss   = run_epoch(model, val_loader, training=False)\n",
    "\n",
    "        print(f\"Epoch {epoch:02d} │ train MSE {train_loss:.4f} │ val MSE {val_loss:.4f}\")\n",
    "    print(\"\\nTraining finished!\")\n",
    "    print(f\"Total time: {time.time() - start_time:.2f} seconds\")\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dct = LinearDCTModel(INPUT_SIZE, OUTPUT_SIZE).to(DEVICE)\n",
    "criterion = nn.MSELoss()\n",
    "optim      = torch.optim.AdamW(model_dct.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 │ train MSE 468490.4352 │ val MSE 412399.0720\n",
      "Epoch 01 │ train MSE 364776.5240 │ val MSE 326219.4917\n",
      "Epoch 02 │ train MSE 287162.4267 │ val MSE 260345.5394\n",
      "Epoch 03 │ train MSE 227945.1441 │ val MSE 209249.8257\n",
      "Epoch 04 │ train MSE 182173.4161 │ val MSE 169169.7791\n",
      "Epoch 05 │ train MSE 146411.7414 │ val MSE 137452.6009\n",
      "Epoch 06 │ train MSE 118230.3621 │ val MSE 112157.3716\n",
      "Epoch 07 │ train MSE 95857.1226 │ val MSE 91860.0961\n",
      "Epoch 08 │ train MSE 77989.2368 │ val MSE 75481.8450\n",
      "Epoch 09 │ train MSE 63645.3261 │ val MSE 62212.3918\n",
      "Epoch 10 │ train MSE 52081.9649 │ val MSE 51418.4421\n",
      "Epoch 11 │ train MSE 42724.7375 │ val MSE 42614.6711\n",
      "Epoch 12 │ train MSE 35128.1999 │ val MSE 35410.8958\n",
      "Epoch 13 │ train MSE 28945.1020 │ val MSE 29505.3041\n",
      "Epoch 14 │ train MSE 23899.3057 │ val MSE 24657.5531\n",
      "Epoch 15 │ train MSE 19773.2426 │ val MSE 20668.2787\n",
      "Epoch 16 │ train MSE 16392.3953 │ val MSE 17380.5744\n",
      "Epoch 17 │ train MSE 13617.4943 │ val MSE 14669.8756\n",
      "Epoch 18 │ train MSE 11336.2021 │ val MSE 12431.0724\n",
      "Epoch 19 │ train MSE 9458.0419 │ val MSE 10581.4028\n",
      "Epoch 20 │ train MSE 7909.8220 │ val MSE 9050.6875\n",
      "Epoch 21 │ train MSE 6631.9199 │ val MSE 7784.3984\n",
      "Epoch 22 │ train MSE 5575.9682 │ val MSE 6735.0681\n",
      "Epoch 23 │ train MSE 4702.3925 │ val MSE 5865.7826\n",
      "Epoch 24 │ train MSE 3979.1186 │ val MSE 5144.3665\n",
      "Epoch 25 │ train MSE 3379.6042 │ val MSE 4545.9736\n",
      "Epoch 26 │ train MSE 2882.3192 │ val MSE 4049.3752\n",
      "Epoch 27 │ train MSE 2469.5149 │ val MSE 3636.7905\n",
      "Epoch 28 │ train MSE 2126.5894 │ val MSE 3294.2149\n",
      "Epoch 29 │ train MSE 1841.5126 │ val MSE 3009.4005\n",
      "Epoch 30 │ train MSE 1604.4874 │ val MSE 2772.6695\n",
      "Epoch 31 │ train MSE 1407.1863 │ val MSE 2575.8715\n",
      "Epoch 32 │ train MSE 1242.9617 │ val MSE 2412.5194\n",
      "Epoch 33 │ train MSE 1106.1584 │ val MSE 2276.7409\n",
      "Epoch 34 │ train MSE 992.2090 │ val MSE 2163.7456\n",
      "Epoch 35 │ train MSE 897.2469 │ val MSE 2070.0936\n",
      "Epoch 36 │ train MSE 818.0182 │ val MSE 1992.2764\n",
      "Epoch 37 │ train MSE 751.9555 │ val MSE 1927.6869\n",
      "Epoch 38 │ train MSE 696.7661 │ val MSE 1874.3341\n",
      "Epoch 39 │ train MSE 650.6536 │ val MSE 1829.9857\n",
      "Epoch 40 │ train MSE 612.1210 │ val MSE 1793.4159\n",
      "Epoch 41 │ train MSE 579.8280 │ val MSE 1763.1601\n",
      "Epoch 42 │ train MSE 552.7188 │ val MSE 1738.5957\n",
      "Epoch 43 │ train MSE 529.9494 │ val MSE 1718.4513\n",
      "Epoch 44 │ train MSE 510.7409 │ val MSE 1701.9571\n",
      "Epoch 45 │ train MSE 494.4761 │ val MSE 1688.5546\n",
      "Epoch 46 │ train MSE 480.6670 │ val MSE 1677.8251\n",
      "Epoch 47 │ train MSE 468.8166 │ val MSE 1669.4288\n",
      "Epoch 48 │ train MSE 458.6704 │ val MSE 1662.9892\n",
      "Epoch 49 │ train MSE 449.8024 │ val MSE 1658.1228\n",
      "Epoch 50 │ train MSE 442.0456 │ val MSE 1654.7496\n",
      "Epoch 51 │ train MSE 435.1369 │ val MSE 1652.1777\n",
      "Epoch 52 │ train MSE 428.9287 │ val MSE 1650.6925\n",
      "Epoch 53 │ train MSE 423.2223 │ val MSE 1650.1540\n",
      "Epoch 54 │ train MSE 417.9141 │ val MSE 1650.7334\n",
      "Epoch 55 │ train MSE 412.9077 │ val MSE 1651.3104\n",
      "Epoch 56 │ train MSE 408.1790 │ val MSE 1652.7512\n",
      "Epoch 57 │ train MSE 403.4765 │ val MSE 1655.0136\n",
      "Epoch 58 │ train MSE 398.9114 │ val MSE 1657.4081\n",
      "Epoch 59 │ train MSE 394.3300 │ val MSE 1660.5344\n",
      "Epoch 60 │ train MSE 389.7425 │ val MSE 1663.5089\n",
      "Epoch 61 │ train MSE 385.0693 │ val MSE 1667.9271\n",
      "Epoch 62 │ train MSE 380.3613 │ val MSE 1672.2065\n",
      "Epoch 63 │ train MSE 375.4292 │ val MSE 1675.7636\n",
      "Epoch 64 │ train MSE 370.4543 │ val MSE 1680.8193\n",
      "Epoch 65 │ train MSE 365.3521 │ val MSE 1685.8659\n",
      "Epoch 66 │ train MSE 360.1003 │ val MSE 1691.2444\n",
      "Epoch 67 │ train MSE 354.7004 │ val MSE 1696.6810\n",
      "Epoch 68 │ train MSE 349.1755 │ val MSE 1702.1000\n",
      "Epoch 69 │ train MSE 343.5095 │ val MSE 1707.9976\n",
      "Epoch 70 │ train MSE 337.6993 │ val MSE 1714.0960\n",
      "Epoch 71 │ train MSE 331.7629 │ val MSE 1720.5949\n",
      "Epoch 72 │ train MSE 325.7359 │ val MSE 1726.9194\n",
      "Epoch 73 │ train MSE 319.5477 │ val MSE 1734.0139\n",
      "Epoch 74 │ train MSE 313.3534 │ val MSE 1740.1076\n",
      "Epoch 75 │ train MSE 307.0107 │ val MSE 1746.7726\n",
      "Epoch 76 │ train MSE 300.5796 │ val MSE 1753.7281\n",
      "Epoch 77 │ train MSE 294.0856 │ val MSE 1759.8804\n",
      "Epoch 78 │ train MSE 287.5421 │ val MSE 1766.9225\n",
      "Epoch 79 │ train MSE 280.9367 │ val MSE 1773.6352\n",
      "Epoch 80 │ train MSE 274.2143 │ val MSE 1780.6870\n",
      "Epoch 81 │ train MSE 267.5409 │ val MSE 1787.1237\n",
      "Epoch 82 │ train MSE 260.8099 │ val MSE 1794.0576\n",
      "Epoch 83 │ train MSE 254.0115 │ val MSE 1801.0747\n",
      "Epoch 84 │ train MSE 247.2707 │ val MSE 1807.4972\n",
      "Epoch 85 │ train MSE 240.4919 │ val MSE 1814.2104\n",
      "Epoch 86 │ train MSE 233.6867 │ val MSE 1821.2335\n",
      "Epoch 87 │ train MSE 226.9276 │ val MSE 1827.9355\n",
      "Epoch 88 │ train MSE 220.2087 │ val MSE 1834.4479\n",
      "Epoch 89 │ train MSE 213.4902 │ val MSE 1841.2954\n",
      "Epoch 90 │ train MSE 206.8001 │ val MSE 1847.3663\n",
      "Epoch 91 │ train MSE 200.1823 │ val MSE 1853.7243\n",
      "Epoch 92 │ train MSE 193.6115 │ val MSE 1860.4892\n",
      "Epoch 93 │ train MSE 187.1006 │ val MSE 1866.4946\n",
      "Epoch 94 │ train MSE 180.6221 │ val MSE 1872.8949\n",
      "Epoch 95 │ train MSE 174.2346 │ val MSE 1879.4593\n",
      "Epoch 96 │ train MSE 168.0179 │ val MSE 1885.1170\n",
      "Epoch 97 │ train MSE 161.9629 │ val MSE 1891.7326\n",
      "Epoch 98 │ train MSE 157.8117 │ val MSE 1901.8488\n",
      "Epoch 99 │ train MSE 155.6425 │ val MSE 1907.2614\n",
      "\n",
      "Training finished!\n",
      "Total time: 83.22 seconds\n"
     ]
    }
   ],
   "source": [
    "train(model_dct, train_loader, val_loader, EPOCHS, optim, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_linear = LinearDCTModel(INPUT_SIZE, OUTPUT_SIZE).to(DEVICE)\n",
    "criterion = nn.MSELoss()\n",
    "optim      = torch.optim.AdamW(model_linear.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 01 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 02 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 03 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 04 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 05 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 06 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 07 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 08 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 09 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 10 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 11 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 12 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 13 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 14 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 15 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 16 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 17 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 18 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 19 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 20 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 21 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 22 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 23 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 24 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 25 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 26 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 27 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 28 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 29 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 30 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 31 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 32 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 33 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 34 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 35 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 36 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 37 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 38 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 39 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 40 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 41 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 42 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 43 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 44 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 45 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 46 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 47 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 48 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 49 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 50 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 51 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 52 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 53 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 54 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 55 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 56 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 57 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 58 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 59 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 60 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 61 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 62 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 63 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 64 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 65 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 66 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 67 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 68 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 69 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 70 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 71 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 72 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 73 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 74 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 75 │ train MSE 144.9701 │ val MSE 1906.6094\n",
      "Epoch 76 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 77 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 78 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 79 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 80 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 81 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 82 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 83 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 84 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 85 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 86 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 87 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 88 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 89 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 90 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 91 │ train MSE 144.9701 │ val MSE 1906.6094\n",
      "Epoch 92 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 93 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 94 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 95 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 96 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 97 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 98 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "Epoch 99 │ train MSE 144.9702 │ val MSE 1906.6094\n",
      "\n",
      "Training finished!\n",
      "Total time: 78.32 seconds\n"
     ]
    }
   ],
   "source": [
    "train(model_linear, train_loader, val_loader, EPOCHS, optim, criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
